# -*- coding: utf-8 -*-
"""Deep Q-Network with Keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p_dwZniOJEkYhhhxN83MUdahEnmXZITR
"""

!pip install gymnasium[classic-control]
#Building a Deep Q-Network with Keras
# implementing a Deep Q-Network (DQN) using Keras to solve a reinforcement learning problem. first set up the environment,
#define the DQN model, train the agent, and evaluate its performance.

import gymnasium as gym
import numpy as np
import random
from collections import deque
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
import warnings

warnings.filterwarnings("ignore")

# Create environment
env = gym.make("CartPole-v1", render_mode=None)
state, info = env.reset(seed=42)

state_size = len(state)
action_size = env.action_space.n

# Build model
def build_model(state_size, action_size):
    model = Sequential([
        Input(shape=(state_size,)),
        Dense(24, activation='relu'),
        Dense(24, activation='relu'),
        Dense(action_size, activation='linear')
    ])
    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))
    return model

model = build_model(state_size, action_size)
memory = deque(maxlen=2000)

epsilon = 1.0
epsilon_min = 0.01
epsilon_decay = 0.995
gamma = 0.95

def act(state):
    global epsilon
    if np.random.rand() <= epsilon:
        return random.randrange(action_size)
    q_values = model.predict(state, verbose=0)
    return np.argmax(q_values[0])

def remember(state, action, reward, next_state, done):
    memory.append((state, action, reward, next_state, done))

def replay(batch_size):
    global epsilon
    minibatch = random.sample(memory, batch_size)
    for state, action, reward, next_state, done in minibatch:
        target = reward
        if not done:
            target = reward + gamma * np.amax(model.predict(next_state, verbose=0)[0])
        target_f = model.predict(state, verbose=0)
        target_f[0][action] = target
        model.fit(state, target_f, epochs=1, verbose=0)
    if epsilon > epsilon_min:
        epsilon *= epsilon_decay

episodes = 50
batch_size = 32

for e in range(episodes):
    state, info = env.reset(seed=42)
    state = np.reshape(state, [1, state_size])
    for time in range(200):
        action = act(state)
        next_state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        next_state = np.reshape(next_state, [1, state_size])
        remember(state, action, reward, next_state, done)
        state = next_state
        if done:
            print(f"Episode: {e+1}/{episodes}, Score: {time}, Epsilon: {epsilon:.2f}")
            break
    if len(memory) > batch_size:
        replay(batch_size)

env.close()