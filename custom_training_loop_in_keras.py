
"""custom training loop in keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lu457CHmpVbudGH9eae9SjjFZZ0tqBEe
"""

# IMPORTS AND ENVIRONMENT SETUP
import os
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Input
from tensorflow.keras.callbacks import Callback
import numpy as np

# Suppress TensorFlow warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


# PART 1: MNIST CUSTOM TRAINING LOOP

# Step 1: Load and preprocess MNIST data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values

# Create batched dataset
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

# Step 2: Define the model
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10)  # Output layer for 10 digits
])

# Step 3: Define loss function, optimizer, and accuracy metric
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam()
accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()


# Step 4: Custom callback
class CustomCallback(Callback):
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        print(f'End of epoch {epoch + 1}, loss: {logs.get("loss")}, accuracy: {logs.get("accuracy")}')


# Step 5: Custom training loop
epochs = 2
custom_callback = CustomCallback()  #runs automatically after each epoch

for epoch in range(epochs):
    print(f'\nStart of epoch {epoch + 1}')

    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        with tf.GradientTape() as tape:
            # Forward pass
            logits = model(x_batch_train, training=True)
            loss_value = loss_fn(y_batch_train, logits)

        # Backward pass (compute and apply gradients)
        grads = tape.gradient(loss_value, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

        # Update accuracy
        accuracy_metric.update_state(y_batch_train, logits)

        # Log progress  #
        if step % 200 == 0:
            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy():.4f}, Accuracy = {accuracy_metric.result().numpy():.4f}')

    # Callback at epoch end
    custom_callback.on_epoch_end(epoch, logs={'loss': loss_value.numpy(), 'accuracy': accuracy_metric.result().numpy()})

    # Reset accuracy metric
    accuracy_metric.reset_state()

# PART 2: SIMPLE BINARY CLASSIFICATION EXAMPLE

# Define model for 20-feature binary classification
binary_model = Sequential([
    Input(shape=(20,)),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')  # Binary output
])

# Compile the model
binary_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Generate random example data
X_train = np.random.rand(1000, 20)  # 1000 samples, 20 features
y_train = np.random.randint(2, size=(1000, 1))  # Binary labels (0 or 1)

# Train the model
binary_model.fit(X_train, y_train, epochs=10, batch_size=32)
